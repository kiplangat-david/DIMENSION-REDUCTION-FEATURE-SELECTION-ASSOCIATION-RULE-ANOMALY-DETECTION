---
title: "FEATURE SELECTION"
author: "KIPLANGAT-DAVID"
date: "February 1, 2022"
output: html_document
---

##RESEARCH QUESTION##

Carrefour Kenya and are currently undertaking a project that will inform the marketing department on the most relevant marketing strategies that will result in the highest no. of sales (total price including tax).This project is aimed at selecting best features for doing analysis on the dataset provided by carrefour and create insights on how to achieve highest sales.

##METRIC FOR SUCCESS##

Be able to detect and do away with anomalies in our dataset

##THE CONTEXT##

Carre Four is an International chain of retail supemarkets in the world, It was set up in Kenya in the year 2016 and has been performing well over the years.Carrefour ensures customer satisfaction and everyday convenience while offering unbeatable value for money with a vast array of more than 100,000 products, shoppers can purchase items for their every need, whether home electronics or fresh fruits from around the world, to locally produced items. This project is aimed at creating insights from existing and current trends to develop marketing strategies that will enable the marketing team achieve higher sales.

##EXPERIMENTAL DESIGN##

1. Loading libraries
2. Load data
3. Data cleaning
4. Feature selection
5. Conclusion
6. Recommendation

##DATA RELEVANCE##

The data we are using for this study was sourced from CarreFour database and is a reflection of current transactions

```{r}
library(caret)
library(corrplot)
```

```{r}
#load data
df <- read.csv('http://bit.ly/CarreFourDataset',stringsAsFactors=F)

#preview top records
tail(df)
```

```{r}
#data shape
dim(df)
```

```{r}
#check missing values
sum(is.na(df))
```

```{r}
#check duplicates
anyDuplicated(df)
```


```{r}
df <- within(df,rm('Invoice.ID'))
```


##FEATURE SELECTION##

##Step wise Forward and Backward Selection##

It searches for the best possible regression model by iteratively selecting and dropping variables to arrive at a model with the lowest possible AIC

Step 1: Define base intercept only model
```{r}
base.mod <- lm(Total ~ 1 , data=df)  
```


Step 2: Full model with all predictors
```{r}
all.mod <- lm(Total ~ . , data= df) 
```


Step 3: Perform step-wise algorithm. direction='both' implies both forward and backward stepwise
```{r}
stepMod <- step(base.mod, scope = list(lower = base.mod, upper = all.mod), direction = "both", trace = 0, steps = 500)  
```

 4: Get and the shortlisted variables.
```{r}
shortlistedVars <- names(unlist(stepMod[[1]])) 
shortlistedVars <- shortlistedVars[!shortlistedVars %in% "(Intercept)"] # remove intercept

# Show
print(shortlistedVars)
```
- Tax and Rating are the best variables



##Recursive Feature Elimination (RFE)##
 
Recursive feature elimnation (rfe) is implemented using the rfe() from caret package. The rfe() takes two important parameters.

 - sizes : determines the number of most important features the rfe should iterate
 - rfeControl : receives the output of the rfeControl()
 
```{r}
subsets <- c(1:5)

ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   verbose = FALSE)

lmProfile <- rfe(x=df[, c(1:3, 5:13)], y=df$Total,
                 sizes = subsets,
                 rfeControl = ctrl)

lmProfile

```



- gross.income, Tax, cogs are the best variables


##CONCLUSIONS#

 - Step wise Forward and Backward Selection method shortlisted tax and rating as its best variables
 
 - Recursive Feature Elimination (RFE) method shortlisted Tax, gross.income as its best variables
 
 
 ##RECOMMENDATION##
 
 More feature selection methods such as lasso regression, Boruta should be implemented and compare with the above methods to obtain real best variables for model training
